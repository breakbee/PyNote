{
 "metadata": {
  "name": "",
  "signature": "sha256:739e5af46fa06a2e7f6abd97be00053935fd29722ccce99ba7fe2fa244452892"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yusuke Sugomori \u3055\u3093\u306eDBN\u5b9f\u88c5\u306e\u30c6\u30b9\u30c8(Python3\u5bfe\u5fdc)  \n",
      "http://blog.yusugomori.com/post/40250499669/python-deep-learning-deep-belief-nets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      " \n",
      "'''\n",
      " Deep Belief Nets (DBN)\n",
      " \n",
      " References :\n",
      "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
      "   Training of Deep Networks, Advances in Neural Information Processing\n",
      "   Systems 19, 2007\n",
      " \n",
      " \n",
      "   - DeepLearningTutorials\n",
      "   https://github.com/lisa-lab/DeepLearningTutorials\n",
      " \n",
      " \n",
      "'''\n",
      " \n",
      "import sys\n",
      "import numpy\n",
      " \n",
      " \n",
      "numpy.seterr(all='ignore')\n",
      " \n",
      "def sigmoid(x):\n",
      "    return 1. / (1 + numpy.exp(-x))\n",
      " \n",
      "def softmax(x):\n",
      "    e = numpy.exp(x - numpy.max(x))  # prevent overflow\n",
      "    if e.ndim == 1:\n",
      "        return e / numpy.sum(e, axis=0)\n",
      "    else:  \n",
      "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # ndim = 2\n",
      " \n",
      " \n",
      "class DBN(object):\n",
      "    def __init__(self, input=None, label=None,\\\n",
      "                 n_ins=2, hidden_layer_sizes=[3, 3], n_outs=2,\\\n",
      "                 numpy_rng=None):\n",
      "        \n",
      "        self.x = input\n",
      "        self.y = label\n",
      " \n",
      "        self.sigmoid_layers = []\n",
      "        self.rbm_layers = []\n",
      "        self.n_layers = len(hidden_layer_sizes)  # = len(self.rbm_layers)\n",
      " \n",
      "        if numpy_rng is None:\n",
      "            numpy_rng = numpy.random.RandomState(1234)\n",
      " \n",
      "        \n",
      "        assert self.n_layers > 0\n",
      " \n",
      " \n",
      "        # construct multi-layer\n",
      "        for i in range(self.n_layers):\n",
      "            # layer_size\n",
      "            if i == 0:\n",
      "                input_size = n_ins\n",
      "            else:\n",
      "                input_size = hidden_layer_sizes[i - 1]\n",
      " \n",
      "            # layer_input\n",
      "            if i == 0:\n",
      "                layer_input = self.x\n",
      "            else:\n",
      "                layer_input = self.sigmoid_layers[-1].sample_h_given_v()\n",
      "                \n",
      "            # construct sigmoid_layer\n",
      "            sigmoid_layer = HiddenLayer(input=layer_input,\n",
      "                                        n_in=input_size,\n",
      "                                        n_out=hidden_layer_sizes[i],\n",
      "                                        numpy_rng=numpy_rng,\n",
      "                                        activation=sigmoid)\n",
      "            self.sigmoid_layers.append(sigmoid_layer)\n",
      " \n",
      " \n",
      "            # construct rbm_layer\n",
      "            rbm_layer = RBM(input=layer_input,\n",
      "                            n_visible=input_size,\n",
      "                            n_hidden=hidden_layer_sizes[i],\n",
      "                            W=sigmoid_layer.W,     # W, b are shared\n",
      "                            hbias=sigmoid_layer.b)\n",
      "            self.rbm_layers.append(rbm_layer)\n",
      " \n",
      " \n",
      "        # layer for output using Logistic Regression\n",
      "        self.log_layer = LogisticRegression(input=self.sigmoid_layers[-1].sample_h_given_v(),\n",
      "                                            label=self.y,\n",
      "                                            n_in=hidden_layer_sizes[-1],\n",
      "                                            n_out=n_outs)\n",
      " \n",
      "        # finetune cost: the negative log likelihood of the logistic regression layer\n",
      "        self.finetune_cost = self.log_layer.negative_log_likelihood()\n",
      " \n",
      " \n",
      " \n",
      "    def pretrain(self, lr=0.1, k=1, epochs=100):\n",
      "        # pre-train layer-wise\n",
      "        for i in range(self.n_layers):\n",
      "            if i == 0:\n",
      "                layer_input = self.x\n",
      "            else:\n",
      "                layer_input = self.sigmoid_layers[i-1].sample_h_given_v(layer_input)\n",
      "            rbm = self.rbm_layers[i]\n",
      "            \n",
      "            for epoch in range(epochs):\n",
      "                rbm.contrastive_divergence(lr=lr, k=k, input=layer_input)\n",
      "                # cost = rbm.get_reconstruction_cross_entropy()\n",
      "                # print >> sys.stderr, \\\n",
      "                #        'Pre-training layer %d, epoch %d, cost ' %(i, epoch), cost\n",
      " \n",
      "    # def pretrain(self, lr=0.1, k=1, epochs=100):\n",
      "    #     # pre-train layer-wise\n",
      "    #     for i in xrange(self.n_layers):\n",
      "    #         rbm = self.rbm_layers[i]\n",
      "            \n",
      "    #         for epoch in xrange(epochs):\n",
      "    #             layer_input = self.x\n",
      "    #             for j in xrange(i):\n",
      "    #                 layer_input = self.sigmoid_layers[j].sample_h_given_v(layer_input)\n",
      "            \n",
      "    #             rbm.contrastive_divergence(lr=lr, k=k, input=layer_input)\n",
      "    #             # cost = rbm.get_reconstruction_cross_entropy()\n",
      "    #             # print >> sys.stderr, \\\n",
      "    #             #        'Pre-training layer %d, epoch %d, cost ' %(i, epoch), cost\n",
      " \n",
      " \n",
      "    def finetune(self, lr=0.1, epochs=100):\n",
      "        layer_input = self.sigmoid_layers[-1].sample_h_given_v()\n",
      " \n",
      "        # train log_layer\n",
      "        epoch = 0\n",
      "        done_looping = False\n",
      "        while (epoch < epochs) and (not done_looping):\n",
      "            self.log_layer.train(lr=lr, input=layer_input)\n",
      "            # self.finetune_cost = self.log_layer.negative_log_likelihood()\n",
      "            # print >> sys.stderr, 'Training epoch %d, cost is ' % epoch, self.finetune_cost\n",
      "            \n",
      "            lr *= 0.95\n",
      "            epoch += 1\n",
      " \n",
      " \n",
      "    def predict(self, x):\n",
      "        layer_input = x\n",
      "        \n",
      "        for i in range(self.n_layers):\n",
      "            sigmoid_layer = self.sigmoid_layers[i]\n",
      "            # rbm_layer = self.rbm_layers[i]\n",
      "            layer_input = sigmoid_layer.output(input=layer_input)\n",
      " \n",
      "        out = self.log_layer.predict(layer_input)\n",
      "        return out\n",
      " \n",
      " \n",
      " \n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, input, n_in, n_out,\\\n",
      "                 W=None, b=None, numpy_rng=None, activation=numpy.tanh):\n",
      "        \n",
      "        if numpy_rng is None:\n",
      "            numpy_rng = numpy.random.RandomState(1234)\n",
      " \n",
      "        if W is None:\n",
      "            a = 1. / n_in\n",
      "            initial_W = numpy.array(numpy_rng.uniform(  # initialize W uniformly\n",
      "                low=-a,\n",
      "                high=a,\n",
      "                size=(n_in, n_out)))\n",
      " \n",
      "            W = initial_W\n",
      " \n",
      "        if b is None:\n",
      "            b = numpy.zeros(n_out)  # initialize bias 0\n",
      " \n",
      " \n",
      "        self.numpy_rng = numpy_rng\n",
      "        self.input = input\n",
      "        self.W = W\n",
      "        self.b = b\n",
      " \n",
      "        self.activation = activation\n",
      " \n",
      "    def output(self, input=None):\n",
      "        if input is not None:\n",
      "            self.input = input\n",
      "        \n",
      "        linear_output = numpy.dot(self.input, self.W) + self.b\n",
      " \n",
      "        return (linear_output if self.activation is None\n",
      "                else self.activation(linear_output))\n",
      " \n",
      "    def sample_h_given_v(self, input=None):\n",
      "        if input is not None:\n",
      "            self.input = input\n",
      " \n",
      "        v_mean = self.output()\n",
      "        h_sample = self.numpy_rng.binomial(size=v_mean.shape,\n",
      "                                           n=1,\n",
      "                                           p=v_mean)\n",
      "        return h_sample\n",
      " \n",
      " \n",
      " \n",
      "class RBM(object):\n",
      "    def __init__(self, input=None, n_visible=2, n_hidden=3, \\\n",
      "        W=None, hbias=None, vbias=None, numpy_rng=None):\n",
      "        \n",
      "        self.n_visible = n_visible  # num of units in visible (input) layer\n",
      "        self.n_hidden = n_hidden    # num of units in hidden layer\n",
      " \n",
      "        if numpy_rng is None:\n",
      "            numpy_rng = numpy.random.RandomState(1234)\n",
      " \n",
      " \n",
      "        if W is None:\n",
      "            a = 1. / n_visible\n",
      "            initial_W = numpy.array(numpy_rng.uniform(  # initialize W uniformly\n",
      "                low=-a,\n",
      "                high=a,\n",
      "                size=(n_visible, n_hidden)))\n",
      " \n",
      "            W = initial_W\n",
      " \n",
      "        if hbias is None:\n",
      "            hbias = numpy.zeros(n_hidden)  # initialize h bias 0\n",
      " \n",
      "        if vbias is None:\n",
      "            vbias = numpy.zeros(n_visible)  # initialize v bias 0\n",
      " \n",
      " \n",
      "        self.numpy_rng = numpy_rng\n",
      "        self.input = input\n",
      "        self.W = W\n",
      "        self.hbias = hbias\n",
      "        self.vbias = vbias\n",
      " \n",
      "        # self.params = [self.W, self.hbias, self.vbias]\n",
      " \n",
      " \n",
      "    def contrastive_divergence(self, lr=0.1, k=1, input=None):\n",
      "        if input is not None:\n",
      "            self.input = input\n",
      "        \n",
      "        ''' CD-k '''\n",
      "        ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
      " \n",
      "        chain_start = ph_sample\n",
      " \n",
      "        for step in range(k):\n",
      "            if step == 0:\n",
      "                nv_means, nv_samples,\\\n",
      "                nh_means, nh_samples = self.gibbs_hvh(chain_start)\n",
      "            else:\n",
      "                nv_means, nv_samples,\\\n",
      "                nh_means, nh_samples = self.gibbs_hvh(nh_samples)\n",
      " \n",
      "        # chain_end = nv_samples\n",
      " \n",
      " \n",
      "        self.W += lr * (numpy.dot(self.input.T, ph_sample)\n",
      "                        - numpy.dot(nv_samples.T, nh_means))\n",
      "        self.vbias += lr * numpy.mean(self.input - nv_samples, axis=0)\n",
      "        self.hbias += lr * numpy.mean(ph_sample - nh_means, axis=0)\n",
      " \n",
      "        # cost = self.get_reconstruction_cross_entropy()\n",
      "        # return cost\n",
      " \n",
      " \n",
      "    def sample_h_given_v(self, v0_sample):\n",
      "        h1_mean = self.propup(v0_sample)\n",
      "        h1_sample = self.numpy_rng.binomial(size=h1_mean.shape,   # discrete: binomial\n",
      "                                       n=1,\n",
      "                                       p=h1_mean)\n",
      " \n",
      "        return [h1_mean, h1_sample]\n",
      " \n",
      " \n",
      "    def sample_v_given_h(self, h0_sample):\n",
      "        v1_mean = self.propdown(h0_sample)\n",
      "        v1_sample = self.numpy_rng.binomial(size=v1_mean.shape,   # discrete: binomial\n",
      "                                            n=1,\n",
      "                                            p=v1_mean)\n",
      "        \n",
      "        return [v1_mean, v1_sample]\n",
      " \n",
      "    def propup(self, v):\n",
      "        pre_sigmoid_activation = numpy.dot(v, self.W) + self.hbias\n",
      "        return sigmoid(pre_sigmoid_activation)\n",
      " \n",
      "    def propdown(self, h):\n",
      "        pre_sigmoid_activation = numpy.dot(h, self.W.T) + self.vbias\n",
      "        return sigmoid(pre_sigmoid_activation)\n",
      " \n",
      " \n",
      "    def gibbs_hvh(self, h0_sample):\n",
      "        v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
      "        h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
      " \n",
      "        return [v1_mean, v1_sample,\n",
      "                h1_mean, h1_sample]\n",
      "    \n",
      " \n",
      "    def get_reconstruction_cross_entropy(self):\n",
      "        pre_sigmoid_activation_h = numpy.dot(self.input, self.W) + self.hbias\n",
      "        sigmoid_activation_h = sigmoid(pre_sigmoid_activation_h)\n",
      "        \n",
      "        pre_sigmoid_activation_v = numpy.dot(sigmoid_activation_h, self.W.T) + self.vbias\n",
      "        sigmoid_activation_v = sigmoid(pre_sigmoid_activation_v)\n",
      " \n",
      "        cross_entropy =  - numpy.mean(\n",
      "            numpy.sum(self.input * numpy.log(sigmoid_activation_v) +\n",
      "            (1 - self.input) * numpy.log(1 - sigmoid_activation_v),\n",
      "                      axis=1))\n",
      "        \n",
      "        return cross_entropy\n",
      " \n",
      "    def reconstruct(self, v):\n",
      "        h = sigmoid(numpy.dot(v, self.W) + self.hbias)\n",
      "        reconstructed_v = sigmoid(numpy.dot(h, self.W.T) + self.vbias)\n",
      "        return reconstructed_v\n",
      " \n",
      " \n",
      "class LogisticRegression(object):\n",
      "    def __init__(self, input, label, n_in, n_out):\n",
      "        self.x = input\n",
      "        self.y = label\n",
      "        self.W = numpy.zeros((n_in, n_out))  # initialize W 0\n",
      "        self.b = numpy.zeros(n_out)          # initialize bias 0\n",
      " \n",
      "    def train(self, lr=0.1, input=None, L2_reg=0.00):\n",
      "        if input is not None:\n",
      "            self.x = input\n",
      " \n",
      "        p_y_given_x = softmax(numpy.dot(self.x, self.W) + self.b)\n",
      "        d_y = self.y - p_y_given_x\n",
      "        \n",
      "        self.W += lr * numpy.dot(self.x.T, d_y) - lr * L2_reg * self.W\n",
      "        self.b += lr * numpy.mean(d_y, axis=0)\n",
      " \n",
      "    def negative_log_likelihood(self):\n",
      "        sigmoid_activation = softmax(numpy.dot(self.x, self.W) + self.b)\n",
      " \n",
      "        cross_entropy = - numpy.mean(\n",
      "            numpy.sum(self.y * numpy.log(sigmoid_activation) +\n",
      "            (1 - self.y) * numpy.log(1 - sigmoid_activation),\n",
      "                      axis=1))\n",
      " \n",
      "        return cross_entropy\n",
      " \n",
      "    def predict(self, x):\n",
      "        return softmax(numpy.dot(x, self.W) + self.b)\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "def test_dbn(pretrain_lr=0.1, pretraining_epochs=1000, k=1, \\\n",
      "             finetune_lr=0.1, finetune_epochs=200):\n",
      " \n",
      "    x = numpy.array([[1,1,1,0,0,0],\n",
      "                     [1,0,1,0,0,0],\n",
      "                     [1,1,1,0,0,0],\n",
      "                     [0,0,1,1,1,0],\n",
      "                     [0,0,1,1,0,0],\n",
      "                     [0,0,1,1,1,0]])\n",
      "    y = numpy.array([[1, 0],\n",
      "                     [1, 0],\n",
      "                     [1, 0],\n",
      "                     [0, 1],\n",
      "                     [0, 1],\n",
      "                     [0, 1]])\n",
      " \n",
      "    \n",
      "    rng = numpy.random.RandomState(123)\n",
      " \n",
      "    # construct DBN\n",
      "    dbn = DBN(input=x, label=y, n_ins=6, hidden_layer_sizes=[3, 3], n_outs=2, numpy_rng=rng)\n",
      " \n",
      "    # pre-training (TrainUnsupervisedDBN)\n",
      "    dbn.pretrain(lr=pretrain_lr, k=1, epochs=pretraining_epochs)\n",
      "    \n",
      "    # fine-tuning (DBNSupervisedFineTuning)\n",
      "    dbn.finetune(lr=finetune_lr, epochs=finetune_epochs)\n",
      " \n",
      " \n",
      "    # test\n",
      "    x = numpy.array([1, 1, 0, 0, 0, 0])\n",
      "    print(dbn.predict(x))\n",
      " \n",
      " \n",
      "if __name__ == \"__main__\":\n",
      "    test_dbn()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.72344411  0.27655589]\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}